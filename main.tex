\documentclass{article}

\usepackage[utf8]{inputenc}
\usepackage[a4paper]{geometry}
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{optidef}
\usepackage{hyperref}
\usepackage{tikz}

\usepackage[
style=authoryear]
{biblatex}
\addbibresource{sources.bib}

\pagestyle{fancy}
\fancyhf{}
\lhead{Ulrik Bernhardt Danielsen}
\rhead{Section \thesection}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}

\title{Specialization Project}
\author{Ulrik Bernhardt Danielsen}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\begin{document}

\maketitle


\section{Task at hand}
Unsupervised clustering of behavioral patterns in rats based on 3D tracking of movements.
Tracking the rats posture has already been proven successful.
My task is to understand this methodology and for my thesis extend it to data including the faces of the rats \textcolor{red}{mainly whiskers?}.


\section{First look at the methodology}
\cite{mimica} presents the decoding of distinct actions in figure \ref{fig:methodology_mimica}.
Section A shows how separate rats were fitted with the probes in two different positions.
\textcolor{red}{Why these positions?} The leftmost figure in section B shows which movements were recorded as time series data.
It records six different movements of the head, neck and back, along with the speed of the rats.
Then the data is \textit{detrended} and \textit{decomposed} spectrally using a Morlet wavelet transform. 
\textcolor{red}{This is maybe where I should start, no idea whats what this means.
The figure shows this as two steps, while the text as one (detrended \textit{using} Morlet).}
The next steps is reducing the dimensionality of the data \textcolor{red}{(wavelets?)}.  
This is done by finding the principal components explaining at least 95\% of the variance, before reducing non-linearly into only two dimensions using t-SNE (t-Stochastic Neighbor Embedding).
Using watershed segmentation on this two-dimensional mapping the discernible actions are found—44 in total.
The final part of the figure, C, shows the decoding accuracy for individual actions across animals.
The decoding accuracy are shown individually for four different cortices. 
\textcolor{red}{This I don't understand.
What are cortices, and what is decoding accuracy?}


\begin{figure}[h]
        \centering
        \includegraphics[width=0.6\linewidth]{./figures/methodology_mimica.png}
        \caption{Figure 1 in \cite{mimica}.}
        \label{fig:methodology_mimica}
\end{figure}

\section{Berman paper}
The part which I will focus on is the methodology developed first in \cite{berman}.
It develops a method for mapping distinct activities in fruit flies.
As stated there: \textit{The basis of our approach is to view behaviour as a trajectory through a high-dimensional space of postural dynamics.}

\subsection{Procedure}
\subsubsection{Postural decomposition}
Since they don't track the flies movements directly, and 40000 timeseries is a bit much, they first apply PCA to Radon transforms of the images.
They find that 50 postural modes are enough to explain a sufficient amount of variance.
\textcolor{red}{To do this they shuffle the dataset and compare the PCA eigenvalues of the data, to the largest one in a shuffled data set.
Is this also the procedure in our case?}
The individual movies can thus be transformed to a 50-dimensional timeseries, which they denote
\begin{equation*}
        \mathbf{Y} \equiv \{ y_1(t), y_2(t), \hdots , y_{50}(t) \}.
\end{equation*}
\textcolor{red}{This step does not seem to be of relevance to us.
I.e., no need to look up Radon transform?}

\subsection{Spectrogram generation}
First they state that looking for repeating sub-sequences in the time series are problematic, as \textit{certain behaviours involve multiple appendages moving at different time scales, this complicating the choice of motif length}.
Thus \cite{berman} chooses another path—a \textit{spectrogram representation for the postural dynamics}.
The Morlet wavelet transform is supposed to be specially suited for dynamics over multiple time scales.
\textcolor{red}{They back this up by citing \textit{Daubechies I. 1992} Ten lectures on wavelets.
Might be a nice resource.}

\subsection{Spacial embedding}
The final step is to map the still very large feature vectors into a low dimensional (two-dimensional) space.
For this t-SNE is chosen.
Why?—because it does care much about preserving the "long" distances between the original features.
Many popular dimensionality reduction methods are tweaked for the opposite purpose, to keep the overall structure in mind.
\textcolor{red}{This argument should definitely be looked more into.}


\section{Steps:}
\begin{enumerate}
        \item z-scoring (standardization \textcolor{red}{although dividing by standard deviation is commented out})
        \item Smoothing by 3d splines
        \item Detrending around the smoothed curve \textcolor{red}{(Why do we need to do this? Do we expect trends?)}
        \item Morlet continuous wavelet transformation
                \begin{enumerate}
                        \item Hypothesis testing against 1st order autoregressive process
                        \item Smoothing of power across scales
                        \item Scale-averaged wavelet power
                        \item Rescaling features
                        \item Concatenating trend data and power spectrum
                \end{enumerate}
        \item \underline{Feature vectors were downsampled in time at 1 Hz and pooled acreoss animals and conditions}. \textcolor{red}{Where is this done in the code?}
        it
        \item PCA, reducing to feature dimension explaining at least 95\% of the variance
        \item t-SNE
        \item Watershed segmentation
\end{enumerate}
\newpage



\section{Introduction}
\subsection{Motivation}
The human brain is an incredibly complex structures that researchers have been trying to understand for a long time.
One way to gain information about how the brain operates is to study its neurons.
Neurons are cells which can communicate with each other through synapses.
This communication are electric signals and can be recorded.
\textcolor{red}{Source?}
At Kavli Institute for Systems Neuroscience at NTNU they are interested in relating these neural spike recordings to the behavior in rats.
This in turn begs the question of how rats behave.
Manually labelling video recordings of rats running around seems a tedious and unfruitful endeavor.
Additionally it introduces bias in our prior assumptions of how the rats behave, and which activities they engage in.
Thus, a methodology for automatically detecting distinct behaviours is needed.

\subsection{Previous work}
\textcolor{red}{Is this necessary?}


\section{Theory}
Concise description of the mathematical concepts used in the methodology.
\subsection{Time series}
We define time series as a realization $y_t = \{ y_{t_1}, y_{t_2}, \hdots, y_{t_n} \}$ of a stochastic process $Y(w, t)$, where $w \in \Omega$, $\Omega$ being the sample space,  and $t \in \mathbb{Z}$, $\mathbb{Z}$ being the chosen index set  \cite{wei}.
It is an ordered series of random variables which can be described completely by its joint probability function
\begin{equation*}
        F_{t_1,\hdots, t_n}(x_1, \hdots, x_n) = \text{Pr}\{ y_{t_1} \leq x_{1}, \hdots, y_{t_n} \leq x_n) \}.
\end{equation*}
The mean and variance function of a time series $y$ are defined as
\begin{equation}\label{eq:mean_func}
        \mu_t = E(y_t)  
\end{equation}
and
\begin{equation*}
        \sigma_t^2 = E(y_t - \mu_t)^2.
\end{equation*}

Given two random variables in the series $y_{t_1}$ and $y_{t_2}$, we define the covariance function and correlation function as
\begin{equation}\label{eq:acv}
        \gamma (t_{1}, t_2) = E[(y_{t_1} - \mu_{t_1})(y_{t_2} - \mu_{t_2})]
\end{equation}
and 
\begin{equation}\label{eq:acf}
        \rho(t_1, t_2) = \frac{\gamma ( t_1, t_2)}{\sqrt{\sigma_{t_1}^2}\sqrt{\sigma_{t_2}^{2}}}.
\end{equation}

\subsubsection{Stationarity}
A time series $y_t$ is $n$th-order stationary if for any shift $h$ and indexes $t_1, t_2, \hdots, t_n$ if 
\begin{equation}\label{eq:nth_stationary}
        F_{y_{t_1}, \hdots, y_{t_n}}(x_1, \hdots, x_n) = F_{y_{t_{1}+h}, \hdots, y_{t_{n} +h}} (x_1, \hdots, x_n).
\end{equation}
If \eqref{eq:nth_stationary} holds for all $n$, the time series is called \textit{strictly} stationary.
We also define a $n$th-order \textit{weakly} stationary time series $y_t$ if the first $n$ joint moments are finite and time invariant.
Specifically we define the second-order weakly stationary, i.e. with constant and time invariant mean function \eqref{eq:mean_func}, and where the covariance function \eqref{eq:acv} is solely a function of the time difference, as \textit{covariance} stationary.
When the covariance function between $t_1, t_2$ can be written as a function of the time difference $h = |t_1 - t_2|$, i.e. $\gamma (t_1, t_2) = \gamma (h) = \gamma_h$, we call it an \textit{autocovariance} function. 
The same is true for the correlation function \eqref{eq:acf}, which when is a function of the time difference is called an \textit{autocorrelation} function (ACF).








\subsubsection{Detrending}
Many methods for analysing and processing time series requires stationarity \cite{shumway}.
If the series is non-stationary, we can split the it into one stationary and one non-stationary part called the \textit{trend}.
Mathematically we write it as 
\begin{equation*}
        y_t = \mu_t + x_t,
\end{equation*}
where $x_t$ denotes the stationary part and $\mu_t$ the trend.
The process of finding $\mu_t$ and then computing $x_t = y_t - \mu_t$ is called \textit{detrending}.
Detecting the trend can be done in many ways, for instance using regression techniques or smoothing.
The simplest way is to assume a linear trend, $\mu_t = \beta_0 + \beta_1 t$ and estimate the parameters using least squares.






\subsubsection{Fourier analysis}
\subsubsection{Spectral analysis}
\subsubsection{Wavelet transformation}
Morlet wavelet—a sine wave that is "windowed" (i.e., multiplied point by point) by a Gaussian

\subsection{Piecewise polynomials}
Suppose we have an interval $[a,b]$ divided into $M$ contiguous subintervals.
The connecting edges of the subintervals $a = \xi_0, \xi_1, \hdots, \xi_{M - 1}, \xi_{M} = b$ are called knots.
On each of the intervals $[\xi_i, \xi_{i+1}], i = 0, \hdots, M-1$ we define a polynomial $p_i (t)$.
The function
\begin{equation*}
        f(t) = 
                \begin{cases}
                        p_0(t), &  t \in [\xi_0, \xi_{1}) \\
                        p_1(t), & t \in [\xi_1, \xi_2)  \\
                        & \vdots \\
                        p_{M-1}(t), & t \in [\xi_{M-1}, \xi_{M}]  \\
                \end{cases}
\end{equation*}
is called a \textit{piecewise polynomial}.


\subsubsection{Splines}
In the definition of piecewise polynomials no restrictions are made on the polynomials, they are allowed to take any form.
As in \cite{quarteroni} we define a \textit{spline} $s_k(t)$ of order $k$ on the interval $[a,b]$ as a piecewise polynomial where
\begin{align*}
        &s_k(t) \in \mathcal{P}^k , \quad t \in [\xi_i, \xi_{i+1}],\quad i = 0, 1, \hdots, M-1 \\
        &s_k(t) \in \mathcal{C}^{k - 1}[a, b].
\end{align*}
I.e., the spline consists of piecewise polynomials of order $k$ and has continuous derivatives up to order $k - 1$.
A common choice is letting $k = 3$, providing continuous second derivatives over the interval.
This is called \textit{cubic} splines, and are often considered sufficiently smooth for function approximations.
It is also common to add curvature constraints at the endpoints, $s_3''(a) = s_3''(b)$, arriving at the \textit{natural} cubic splines.

\subsubsection{Regression splines}
Suppose now we have data points $y_{t_1}, y_{t_2}, \hdots, y_{t_n}$ on $[a = t_1, b = t_n]$. 
A spline of order $k$ with chosen knots at $a = t_1 = \xi_0, \xi_1, \hdots, \xi_{M} = t_n = b$ can be parameterized as 
\begin{equation*}
        s_k(t) = \sum_{i = 1}^{M + K} \beta_i h_i(t),
\end{equation*}
where the functions $h_i$ are the truncated-power basis set
\begin{align*}
        h_j(t) &= t^{j - 1}, \ j = 1, \hdots, k+1, \\
        h_{k+1+l}(t) &= (t - \xi_l)_+^k, \ l = 1, \hdots, M-1,
\end{align*}
with $(t)_+ = \max_{} \{ t, 0 \}$ \cite{hastie}.
The parameters $\beta_i$ can be found using least squares.





\subsection{Dimensionality reduction}
\subsubsection{Principal Component Analysis}
\subsubsection{t-Stochastic Neighbor Embedding}

\subsection{Watershed segmentation}

\section{Methodology}
What is done in practice.
Discussion  of choices made.
\subsection{Input data}
The starting point of the analysis is a collection of $n$ separate time series of equal length $m$ \textcolor{red}{right?}.
We denote these $\textbf{Y} = \{ y_1(t), \hdots, y_n(t) \}$, where $t \in \{ t_1, t_2, \hdots, t_m \}$.
\subsection{Feature extraction}
\subsection{Manifold embedding}








\newpage
\printbibliography
\end{document}



