\documentclass[a4paper]{memoir}

\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{amsmath, amsfonts, amssymb, amsthm, mathtools}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{optidef}
\usepackage{hyperref}
\usepackage{tikz}

\usepackage[
style=authoryear]
{biblatex}
\addbibresource{sources.bib}


\chapterstyle{ger}
\maxsecnumdepth{subsection}
\maxtocdepth{subsection}

\pagestyle{fancy}
\fancyhf{}
\lhead{\rightmark}
\rhead{\leftmark}
\rfoot{Page \thepage \hspace{1pt} of \pageref{LastPage}}

\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}

\title{Specialization Project}
\author{Ulrik Bernhardt Danielsen}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{proposition}{Proposition}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]

\theoremstyle{remark}
\newtheorem*{remark}{Remark}

\begin{document}

\maketitle

\tableofcontents*
\clearpage


\section*{Steps:}
\begin{enumerate}
        \item z-scoring (standardization \textcolor{red}{although dividing by standard deviation is commented out})
        \item Smoothing by 3d splines
        \item Detrending around the smoothed curve \textcolor{red}{(Why do we need to do this? Do we expect trends?)}
        \item Morlet continuous wavelet transformation
                \begin{enumerate}
                        \item Hypothesis testing against 1st order autoregressive process
                        \item Smoothing of power across scales
                        \item Scale-averaged wavelet power
                        \item Rescaling features
                        \item Concatenating trend data and power spectrum
                \end{enumerate}
        \item \underline{Feature vectors were downsampled in time at 1 Hz and pooled acreoss animals and conditions}. \textcolor{red}{Where is this done in the code?}
        it
        \item PCA, reducing to feature dimension explaining at least 95\% of the variance
        \item t-SNE
        \item Watershed segmentation
\end{enumerate}
\newpage



\chapter{Introduction}
\section{Motivation}
The human brain is an incredibly complex structures that researchers have been trying to understand for a long time.
One way to gain information about how the brain operates is to study its neurons.
Neurons are cells which can communicate with each other through synapses.
This communication are electric signals and can be recorded.
\textcolor{red}{Source?}
At Kavli Institute for Systems Neuroscience at NTNU they are interested in relating these neural spike recordings to the behavior in rats.
This in turn begs the question of how rats behave.
Manually labelling video recordings of rats running around seems a tedious and unfruitful endeavor.
Additionally it introduces bias in our prior assumptions of how the rats behave, and which activities they engage in.
Thus, a methodology for automatically detecting distinct behaviours is needed.

\section{Previous work}
\textcolor{red}{Is this necessary?}



\chapter{Theory}
\section{Time series analysis}
We define time series as a realization $y_t = \{ y_{t_1}, y_{t_2}, \hdots, y_{t_n} \}$ of a stochastic process $Y(w, t)$, where $w \in \Omega$, $\Omega$ being the sample space,  and $t \in \mathbb{Z}$, $\mathbb{Z}$ being the chosen index set  \cite{wei}.
It is an ordered series of random variables which can be described completely by its joint probability function
\begin{equation*}
        F_{t_1,\hdots, t_n}(x_1, \hdots, x_n) = \text{Pr}\{ y_{t_1} \leq x_{1}, \hdots, y_{t_n} \leq x_n) \}.
\end{equation*}
The mean and variance function of a time series $y$ are defined as
\begin{equation}\label{eq:mean_func}
        \mu_t = E(y_t)  
\end{equation}
and
\begin{equation*}
        \sigma_t^2 = E(y_t - \mu_t)^2.
\end{equation*}

Given two random variables in the series $y_{t_1}$ and $y_{t_2}$, we define the covariance function and correlation function as
\begin{equation}\label{eq:acv}
        \gamma (t_{1}, t_2) = E[(y_{t_1} - \mu_{t_1})(y_{t_2} - \mu_{t_2})]
\end{equation}
and 
\begin{equation}\label{eq:acf}
        \rho(t_1, t_2) = \frac{\gamma ( t_1, t_2)}{\sqrt{\sigma_{t_1}^2}\sqrt{\sigma_{t_2}^{2}}}.
\end{equation}



\subsection{Stationarity}
A time series $y_t$ is $n$th-order stationary if for any shift $h$ and indexes $t_1, t_2, \hdots, t_n$ if 
\begin{equation}\label{eq:nth_stationary}
        F_{y_{t_1}, \hdots, y_{t_n}}(x_1, \hdots, x_n) = F_{y_{t_{1}+h}, \hdots, y_{t_{n} +h}} (x_1, \hdots, x_n).
\end{equation}
If \eqref{eq:nth_stationary} holds for all $n$, the time series is called \textit{strictly} stationary.
We also define a $n$th-order \textit{weakly} stationary time series $y_t$ if the first $n$ joint moments are finite and time invariant.
Specifically we define the second-order weakly stationary, i.e. with constant and time invariant mean function \eqref{eq:mean_func}, and where the covariance function \eqref{eq:acv} is solely a function of the time difference, as \textit{covariance} stationary.
When the covariance function between $t_1, t_2$ can be written as a function of the time difference $h = |t_1 - t_2|$, i.e. $\gamma (t_1, t_2) = \gamma (h) = \gamma_h$, we call it an \textit{autocovariance} function. 
The same is true for the correlation function \eqref{eq:acf}, which when is a function of the time difference is called an \textit{autocorrelation} function (ACF).
Figure \ref{fig:time_series_example} shows an example of a time series.
As the mean seem to increase with $t$ it is non-stationary.

\begin{figure}[tb]
        \centering
        \includegraphics[width=\linewidth]{./code/figures/time_series_example.pdf}
        \caption{Example of a non-stationary time series with $t_0 = 0, t_n = 4$.
        The discrete data points are connected to better visualize the movement through time.}
        \label{fig:time_series_example}
\end{figure}

We also define the common estimates for the mean and covariance functions.
They are called the sample mean and sample covariance, and are written as
\begin{align}\label{eq:sample_mean}
        \bar{y_t} &= \frac{1}{n}\sum_{i = 1}^{n} y_{t_i}, \\
        \label{eq:sample_covariance}
        \hat{\gamma}_h &= \frac{1}{n} \sum_{i = 1}^{n - h}(y_{t_i} - \bar{y_t})(y_{t_i  + h} - \bar{y_t}),
\end{align}
respectively.


\subsection{Detrending}
Many methods for analysing and processing time series requires stationarity \cite{shumway}.
If the series is non-stationary, we can split the it into one stationary and one non-stationary part called the \textit{trend}.
Mathematically we write it as 
\begin{equation*}
        y_t = \mu_t + x_t,
\end{equation*}
where $x_t$ denotes the stationary part and $\mu_t$ the trend.
The process of finding $\mu_t$ and then computing $x_t = y_t - \mu_t$ is called \textit{detrending}.
Detecting the trend can be done in many ways, for instance using regression techniques or smoothing.
The simplest way is to assume a linear trend, $\mu_t = \beta_0 + \beta_1 t$ and estimate the parameters using least squares.
In figure \ref{fig:time_series_example_with_trend} the linear regression fit is shown, showing an upwards in the time series.


\begin{figure}[tb]
        \centering
        \includegraphics[width=\linewidth]{./code/figures/time_series_example_with_trend.pdf}
        \caption{Time series from figure \ref{fig:time_series_example} with an estimated linear trend shown in blue.}
        It is clear that there exists an upward trend in the data.
        \label{fig:time_series_example_with_trend}
\end{figure}




\section{Fourier analysis}
Let $Z_1, Z_2, \hdots, Z_n$ be a sequence of numbers.
For simplicity in notation we assume $n$ to be an odd number.
It can be shown that the sequence can be represented as a linear combination of complex exponentials
\begin{equation}\label{eq:fourier_series}
        Z_t = \sum_{k = -\frac{n-1}{2}}^{\frac{n- 1}{2}}c_k e^{\frac{i2 \pi k t}{n}}.
\end{equation}
This comes from the fact that the set 
\begin{equation*}
        \left\{ e^{\frac{i 2 \pi kt}{n}} \Big| k \in \left[ -\frac{n -1}{2}, \frac{n - 1}{2} \right] \right\}
\end{equation*}
consists of $n$ orthogonal functions \cite{wei}.
I.e., that
\begin{equation*}
        \sum_{i = 1}^{n} e^{\frac{i2 \pi kt}{n}} e^{-\frac{i 2 \pi j t}{n}}= 
                \begin{cases}
                        n, & k = j \\
                        0, & k \neq j \\
                \end{cases}
       .
\end{equation*}
The coefficients $c_k$ are given by
\begin{equation*}
       c_k =  \frac{1}{n}\sum_{ t = 1}^{n}Z_t e^{-\frac{i 2\pi kt}{n}}.
\end{equation*}
It is clear that \eqref{eq:fourier_series} is periodic with period $n$, meaning $Z_{t + jn} = Z_t, j = 0, \pm 1, \pm 2, \hdots$.
Thus the Fourier series is able to capture periodic sequences.
The smallest positive integer $n$ for which $Z_{t + n} = Z_t$ is called the fundamental period, with corresponding fundamental frequency $2 \pi /n$.
For the components $k = \pm j, j = 1, 2, \hdots, (n-1)/2$ the frequencies are multiples of the fundamental frequency, $w_k = k(2\pi/n)$.
The set of frequencies making up the series is called the \textit{spectrum}.
As a consequence the coefficients $c_k$ can be viewed as weighting the importance of the contributions for the different frequencies making up the full sequence.
This is formalized by the definitions of energy and power,
\begin{align}\label{eq:energy}
        \text{energy} &= \sum_{t = 2}^{n} Z_t^2 = n \sum_{k = -\frac{n -1}{2}}^{\frac{n - 1}{2}} |c_k|^2 ,\\ 
        \label{eq:power}
        \text{power} = \frac{\text{energy}}{n} &= \sum_{k = -\frac{n -1}{2}}^{\frac{n - 1}{2}} |c_k|^2. 
\end{align}
Let $p_k$ be the contribution to the power from frequency $k = 0, 1, \hdots, (n-1)/2$.
As $w_k$ and $w_{-k}$ corresponds to the same frequency the contribution is given as $p_0 = c_0^2, p_k = 2|c_k|^2, k = 1, \hdots, (n-1)/2$.
The values $p_k$ are called the power spectrum of the series.


\subsection{Discrete-Time Fourier Transform}
We have seen that all sequences of length $n$ can be viewed and parameterized as Fourier series with period $n$.
Moving to non-periodic sequences essentially amounts to taking the limit of the series as $n$ approaches infinity.
Formally we now let $Z_t$ be a finite discrete function of $t$, where $Z_t = 0$ when $|t| > M$ for some integer $M$.
Choosing $n = 2M + 1$ the function
\begin{equation*}
        Y_{t + jn} = Z_t, \ t \in \left[ -\frac{n-1}{2}, \frac{n-1}{2} \right], \ j \in \mathbb{Z}
\end{equation*}
is periodic with period $n$.
It's Fourier series is 
\begin{equation*}
        Y_t = \sum_{k = -\frac{n - 1}{2}}^{\frac{n -1 }{2}} c_k e^{\frac{i2 \pi kt}{n}}.
\end{equation*}
As $Y_t = Z_t$ when $t \in [-(n-1)/2, (n-1)/2]$, and $Z_t = 0$ when $|t| > (n-1)/2$, the coefficients $c_k$ can be written as the infinite sum
\begin{align*}
        c_k &= \frac{1}{n}\sum_{t = - \infty}^{\infty} Z_t e^{\frac{-i2\pi kt}{n}} \\
        & = \frac{2 \pi}{n}f\left(\frac{2\pi k}{n}\right),
\end{align*}
where 
\begin{equation*}
        f(w) = \frac{1}{2\pi} \sum_{t = -\infty}^{\infty} Z_t e^{-iwt}.
\end{equation*}
If we now take the limit $Z_t = \lim_{n \rightarrow \infty} Y_t$ the summation becomes an integral over the length $2\pi$ \cite{wei}.
This gives the relation
\begin{align}\label{eq:dtft_inv}
        Z_t &= \int_{-\pi}^{\pi} f(w) e^{iwt} dw, \quad t \in \mathbb{Z} \\
        \label{eq:dtft}
        f(w) &= \frac{1}{2 \pi} \sum_{t = -\infty}^{\infty} Z_t e^{-i w t}, \quad -\pi \leq w \leq \pi,
\end{align}
where $f(w)$ in \eqref{eq:dtft} is called the discrete-time Fourier transform of $Z_t$.
As opposed to the periodic case \eqref{eq:fourier_series} where the periodic sequence was made up by a finite number of frequencies, the non-periodic sequence is an integral over a continuum of frequencies $w$.
We call $|f(w)|$ the spectrum of the sequence, and the function $g(w) = 2 \pi |f(w)|^2$ the energy spectrum.
The energy spectrum definition comes from Parseval's relation 
\begin{equation}\label{eq:parseval}
        \sum_{t = -\infty}^{\infty} |Z_t|^2 = 2 \pi \int_{- \pi}^{\pi}|f(w)|^2 dw.
\end{equation}
It is worth noting that the relation in \eqref{eq:parseval} only holds when the sequence $Z_t$ is absolutely summable, i.e., that
\begin{equation}\label{eq:absuletly_summable}
        \sum_{ t = -\infty}^{\infty}|Z_t| < \infty.
\end{equation}

\section{Spectral analysis}
Let $y_t$ be a stationary time series where the autocovariance function $\gamma_h$ from \eqref{eq:acv} is absolutely summable.
Then we can write $\gamma_h$ as a Fourier transform pair
\begin{align}\label{eq:spectrum}
        f(w) &= \frac{1}{2 \pi} \sum_{h = -\infty}^{\infty} \gamma_h e^{-wh}, \\
        \label{eq:spectral_dens_inv}
        \gamma_h &= \int_{-\pi}^{\pi} f(w)e^{iwh}dw.
\end{align}
It can be shown \cite{shumway} that the spectrum $f(w)$ in \eqref{eq:spectrum} is real-valued and non-negative.
Furthermore, as $\text{Var}(y_t) = \gamma_0$, we get the interpretation
\begin{equation*}
        \text{Var}(y_t) = \int_{-\pi}^{\pi}f(w)dw,
\end{equation*}
i.e., that $f(w)$ is the contribution to the variance for frequency $w$.
To locate these important frequencies is what we often seek, and thus an important task is to estimate this spectrum.

\subsection{Periodogram}
Again we consider a time series sample $y_1, y_2, \hdots$ where $n$ is chosen to be odd for readability.
It can be written as a real Fourier representation 
\begin{equation*}
        y_t = a_0 + \sum_{k = 1}^{\frac{n-1}{2}} (a_k \cos(w_k t) + b_k \sin (w_kt)),
\end{equation*}
where $w_k = 2\pi k / n, k =0, 1, \hdots, (n -1)/2$ and the coefficients are given as 
\begin{equation*}
        a_0 = \bar{y_t}, \quad a_k = \frac{2}{n}\sum_{t = 1}^{n}y_t \cos(w_kt), \quad b_k = \frac{2}{n} \sum_{ t = 1}^{n} y_t \sin (w_kt).
\end{equation*}
We then define the periodogram as 
\begin{equation}\label{eq:periodogram}
        I(w_k) = 
                \begin{cases}
                        na_0^2, & k=0  \\
                        \frac{n}{2}(a_k^2 + b_k^2), & k = 1, \hdots, (n-1)/2. \\
                \end{cases}
\end{equation}
The periodogram is of interest as it has has a large value if the frequency $w_k$ is of importance in the series.
A scaled periodogram $\frac{2}{n}I(w_k)$ estimates the sample variance of the sinusoid component at frequency $w_k$ \cite{shumway}.




\subsection{Sample spectrum}
One intuitive way of estimating the spectrum is to replace the autocovariance with the sample autocovariance from equation \eqref{eq:sample_covariance}.
I.e., we define the sample spectrum for a realization $y_1, y_2,  \hdots, y_n$
\begin{equation}\label{eq:sample_spectrum}
        \hat{f}(w) = \frac{1}{2\pi}\sum_{h = -(n - 1)}^{n-1}\hat{\gamma}_h e^{-iwh}.
\end{equation}
At the Fourier frequencies $w_k$ it is related to the periodogram through \cite{wei}
\begin{equation*}
        \hat{f}(w_k) = \frac{I(w_k)}{4 \pi}.
\end{equation*}
Although $f(w_k)$ is asymptotically unbiased, meaning $\lim_{n \rightarrow \infty} E(\hat{f}(w)) = f(w)$, is lacks consistency in the variance as $n$ tends to infinity, i.e.,
\begin{equation*}
        \lim_{n \rightarrow \infty} \text{Var}(\hat{f}(w_k)) \neq 0.
\end{equation*}
Thus  we consider the sample spectrum 


\subsection{Spectral Window}





\subsection{Short-Time Fourier Transform}






\subsection{Wavelet transformation}
Morlet waveletâ€”a sine wave that is "windowed" (i.e., multiplied point by point) by a Gaussian

\section{Piecewise polynomials}
Suppose we have an interval $[a,b]$ divided into $M$ contiguous subintervals.
The connecting edges of the subintervals $a = \xi_0, \xi_1, \hdots, \xi_{M - 1}, \xi_{M} = b$ are called knots.
On each of the intervals $[\xi_i, \xi_{i+1}], i = 0, \hdots, M-1$ we define a polynomial $p_i (t)$.
The function
\begin{equation*}
        f(t) = 
                \begin{cases}
                        p_0(t), &  t \in [\xi_0, \xi_{1}) \\
                        p_1(t), & t \in [\xi_1, \xi_2)  \\
                        & \vdots \\
                        p_{M-1}(t), & t \in [\xi_{M-1}, \xi_{M}]  \\
                \end{cases}
\end{equation*}
is called a \textit{piecewise polynomial}.


\subsection{Splines}
In the definition of piecewise polynomials no restrictions are made on the polynomials, they are allowed to take any form.
As in \cite{quarteroni} we define a \textit{spline} $s_k(t)$ of order $k$ on the interval $[a,b]$ as a piecewise polynomial where
\begin{align*}
        &s_k(t) \in \mathcal{P}^k , \quad t \in [\xi_i, \xi_{i+1}],\quad i = 0, 1, \hdots, M-1 \\
        &s_k(t) \in \mathcal{C}^{k - 1}[a, b].
\end{align*}
I.e., the spline consists of piecewise polynomials of order $k$ and has continuous derivatives up to order $k - 1$.
A common choice is letting $k = 3$, providing continuous second derivatives over the interval.
This is called \textit{cubic} splines, and are often considered sufficiently smooth for function approximations.
It is also common to add curvature constraints at the endpoints, $s_3''(a) = s_3''(b)$, arriving at the \textit{natural} cubic splines.

\subsection{Regression splines}
Suppose now we have data points $y_{t_1}, y_{t_2}, \hdots, y_{t_n}$ on $[a = t_1, b = t_n]$. 
A spline of order $k$ with chosen knots at $a = t_1 = \xi_0, \xi_1, \hdots, \xi_{M} = t_n = b$ can be parameterized as 
\begin{equation*}
        s_k(t) = \sum_{i = 1}^{M + K} \beta_i h_i(t),
\end{equation*}
where the functions $h_i$ are the truncated-power basis set
\begin{align*}
        h_j(t) &= t^{j - 1}, \ j = 1, \hdots, k+1, \\
        h_{k+1+l}(t) &= (t - \xi_l)_+^k, \ l = 1, \hdots, M-1,
\end{align*}
with $(t)_+ = \max_{} \{ t, 0 \}$ \cite{hastie}.
The parameters $\beta_i$ can be found using least squares.
An example of cubic spline regression are shown in figure \ref{fig:cubic_splines}.

\begin{figure}[tb ]
        \centering
        \includegraphics[width=\linewidth]{./code/figures/cubic_splines.pdf}
        \caption{Two cubic splines fitted using least square regression on the time series from figure \ref{fig:time_series_example}.
        Observe that the red spline with 50 knots (including endpoints) fits the data closer than the green spline with 10 knots.}
        \label{fig:cubic_splines}
\end{figure}




\section{Dimensionality reduction}
\subsection{Principal Component Analysis}
\subsection{t-Stochastic Neighbor Embedding}
\section{Watershed segmentation}




\chapter{Methodology}
The starting point of the analysis is a set $\textbf{Y} =  \{ Y_d : d = 1, 2, \hdots, D \}$, where each set $Y_d$ is again a set of independent time series, $Y_d = \{ y_1^d(t), y_2^d(t), \hdots, y_n^d(t): t \in \{t_1, t_2, \hdots, t_{m_d} \} \}$.
Each $Y_d$ represents an animal for which $n$ time series are collected tracking parts of its movements.
Recording frequency is the same across animals.
The goal of the analysis is to cluster these time points into distinct distinguishable actions.

\textcolor{red}{Some summarization: "This is achieved by..."}

\section{Feature extraction}
The first step is to analyse each time series separately, modifying them and thus creating $D$

\section{Manifold embedding}








\newpage
\printbibliography
\end{document}



